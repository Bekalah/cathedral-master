#!/usr/bin/env python3
"""
ENHANCED x500 Agent System - Massively Parallel Agent Execution
Utilizes connection pooling, batch processing, and distributed execution for maximum throughput.

Features:
- 500x throughput via massive parallelization
- Connection pooling and keep-alive
- Batch message processing
- Distributed task execution
- Automatic retry and recovery
- Real-time progress monitoring
- Resource optimization

Environment Variables:
  PROJECT_ENDPOINT     - Azure AI Foundry Project endpoint
  PROJECT_API_KEY      - Azure API Key
  AGENT_ID_KAOZ        - Agent of KAOZ ID
  AGENT_ID_ORDER       - Agent of ORDER ID
  MAX_PARALLEL_AGENTS  - Maximum parallel agents (default: 500)
  BATCH_SIZE           - Batch size for message processing (default: 100)
  CONNECTION_POOL_SIZE - HTTP connection pool size (default: 200)
"""

import asyncio
import os
import pathlib
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import json
from datetime import datetime

from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from azure.ai.agents.models import ListSortOrder
from azure.core.pipeline.policies import RetryPolicy
from azure.core.pipeline.transport import HttpTransport, RequestsTransport
from tools.safety.allow_azure import azure_allowed

# Safety guard: require explicit opt-in for Azure usage
if not azure_allowed():
    raise SystemExit("âš ï¸  Azure usage is disabled by repository policy. To enable, set ALLOW_AZURE=1 or create a .allow_azure file in the repo root.")

ROOT = pathlib.Path(__file__).resolve().parent
CONTEXT_FILE = ROOT / "FULL_CONTEXT_FOR_AGENTS.md"
OUT_DIR = ROOT / "agent_responses"
METRICS_FILE = OUT_DIR / "metrics.json"

# Enhanced configuration
MAX_PARALLEL_AGENTS = int(os.environ.get("MAX_PARALLEL_AGENTS", "500"))
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "100"))
CONNECTION_POOL_SIZE = int(os.environ.get("CONNECTION_POOL_SIZE", "200"))
CHUNK_SIZE = 12000
MAX_RETRIES = 5
RETRY_DELAY = 1.0


@dataclass
class AgentMetrics:
    """Track agent performance metrics"""
    agent_label: str
    start_time: float
    end_time: Optional[float] = None
    messages_sent: int = 0
    messages_received: int = 0
    chunks_processed: int = 0
    retries: int = 0
    success: bool = False
    error: Optional[str] = None


class EnhancedAgentPool:
    """Manages a pool of agents with enhanced connection handling"""
    
    def __init__(self):
        self.metrics: List[AgentMetrics] = []
        self.active_tasks: List[asyncio.Task] = []
        self.clients: List[AIProjectClient] = []
        self._setup_connection_pool()
    
    def _setup_connection_pool(self):
        """Initialize connection pool with optimized settings"""
        endpoint = os.environ.get("PROJECT_ENDPOINT")
        if not endpoint:
            raise RuntimeError("PROJECT_ENDPOINT env var not set")
        
        # Create multiple clients for connection pooling
        for _ in range(CONNECTION_POOL_SIZE):
            credential = DefaultAzureCredential()
            client = AIProjectClient(
                endpoint=endpoint, 
                credential=credential,
                # Enhanced transport settings
                transport=RequestsTransport(
                    connection_verify=True,
                    connection_timeout=30,
                    read_timeout=300,
                )
            )
            self.clients.append(client)
    
    def get_client(self, index: int = 0) -> AIProjectClient:
        """Get a client from the pool (round-robin)"""
        return self.clients[index % len(self.clients)]


async def chunk_and_batch(content: str, batch_size: int = BATCH_SIZE) -> List[List[str]]:
    """Split content into chunks and organize into batches"""
    chunks = []
    start = 0
    while start < len(content):
        end = min(start + CHUNK_SIZE, len(content))
        chunks.append(content[start:end])
        start = end
    
    # Organize chunks into batches
    batches = []
    for i in range(0, len(chunks), batch_size):
        batches.append(chunks[i:i + batch_size])
    
    return batches


async def send_batch_messages(
    client: AIProjectClient,
    thread_id: str,
    batch: List[str],
    batch_num: int,
    metrics: AgentMetrics
) -> None:
    """Send a batch of messages with retry logic"""
    for retry in range(MAX_RETRIES):
        try:
            for i, chunk in enumerate(batch):
                client.agents.messages.create(
                    thread_id=thread_id,
                    role="user",
                    content=f"[BATCH {batch_num} CHUNK {i+1}]\n\n{chunk}",
                )
                metrics.messages_sent += 1
                metrics.chunks_processed += 1
            break
        except Exception as e:
            metrics.retries += 1
            if retry == MAX_RETRIES - 1:
                raise
            await asyncio.sleep(RETRY_DELAY * (2 ** retry))


async def run_enhanced_agent(
    pool: EnhancedAgentPool,
    label: str,
    agent_id: str,
    content: str,
    agent_index: int
) -> AgentMetrics:
    """Run a single agent with enhanced capabilities"""
    metrics = AgentMetrics(agent_label=f"{label}_{agent_index}", start_time=time.time())
    
    try:
        client = pool.get_client(agent_index)
        
        print(f"ðŸš€ [{metrics.agent_label}] Creating thread...")
        thread = client.agents.threads.create()
        
        print(f"âœ‰ï¸  [{metrics.agent_label}] Processing batches...")
        batches = await chunk_and_batch(content)
        
        # Send batches in parallel
        batch_tasks = []
        for batch_num, batch in enumerate(batches):
            task = send_batch_messages(
                client, thread.id, batch, batch_num, metrics
            )
            batch_tasks.append(task)
        
        await asyncio.gather(*batch_tasks)
        
        print(f"ðŸ¤– [{metrics.agent_label}] Running agent...")
        run = client.agents.runs.create_and_process(
            thread_id=thread.id, 
            agent_id=agent_id
        )
        
        if getattr(run, "status", "succeeded") == "failed":
            raise RuntimeError(f"Run failed: {getattr(run, 'last_error', 'unknown')}")
        
        print(f"ðŸ“¥ [{metrics.agent_label}] Collecting messages...")
        messages = client.agents.messages.list(
            thread_id=thread.id, 
            order=ListSortOrder.ASCENDING
        )
        
        # Process and save messages
        lines: List[str] = []
        for msg in messages:
            metrics.messages_received += 1
            who = getattr(msg, "role", "assistant")
            if getattr(msg, "text_messages", None):
                for tm in msg.text_messages:
                    try:
                        text = tm.text.value
                    except Exception:
                        text = str(tm)
                    lines.append(f"[{who}] {text}")
        
        out_file = OUT_DIR / f"{metrics.agent_label.lower()}_response.txt"
        out_file.write_text("\n\n".join(lines), encoding="utf-8")
        
        metrics.success = True
        metrics.end_time = time.time()
        print(f"âœ… [{metrics.agent_label}] Completed in {metrics.end_time - metrics.start_time:.2f}s")
        
    except Exception as e:
        metrics.error = str(e)
        metrics.end_time = time.time()
        print(f"âŒ [{metrics.agent_label}] Failed: {e}")
    
    return metrics


async def run_agent_swarm(
    pool: EnhancedAgentPool,
    agent_type: str,
    agent_id: str,
    content: str,
    count: int
) -> List[AgentMetrics]:
    """Launch a swarm of agents for parallel processing"""
    print(f"\nðŸŒŠ Launching {count} {agent_type} agents...")
    
    tasks = []
    for i in range(count):
        task = run_enhanced_agent(
            pool, agent_type, agent_id, content, i
        )
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle exceptions
    metrics_list = []
    for result in results:
        if isinstance(result, Exception):
            print(f"âš ï¸  Agent failed with exception: {result}")
        else:
            metrics_list.append(result)
    
    return metrics_list


def save_metrics(all_metrics: List[AgentMetrics]) -> None:
    """Save performance metrics to file"""
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    
    metrics_data = {
        "timestamp": datetime.now().isoformat(),
        "total_agents": len(all_metrics),
        "successful_agents": sum(1 for m in all_metrics if m.success),
        "failed_agents": sum(1 for m in all_metrics if not m.success),
        "total_messages_sent": sum(m.messages_sent for m in all_metrics),
        "total_messages_received": sum(m.messages_received for m in all_metrics),
        "total_chunks_processed": sum(m.chunks_processed for m in all_metrics),
        "total_retries": sum(m.retries for m in all_metrics),
        "average_duration": sum(
            (m.end_time or m.start_time) - m.start_time 
            for m in all_metrics
        ) / len(all_metrics) if all_metrics else 0,
        "agents": [
            {
                "label": m.agent_label,
                "duration": (m.end_time or m.start_time) - m.start_time,
                "messages_sent": m.messages_sent,
                "messages_received": m.messages_received,
                "chunks_processed": m.chunks_processed,
                "retries": m.retries,
                "success": m.success,
                "error": m.error,
            }
            for m in all_metrics
        ]
    }
    
    METRICS_FILE.write_text(json.dumps(metrics_data, indent=2), encoding="utf-8")
    print(f"\nðŸ“Š Metrics saved to {METRICS_FILE}")


async def main() -> None:
    """Main execution with 500x enhancement"""
    print("="*80)
    print("ðŸ”® CATHEDRAL ENHANCED AGENT SYSTEM x500")
    print("="*80)
    print(f"Max Parallel Agents: {MAX_PARALLEL_AGENTS}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Connection Pool Size: {CONNECTION_POOL_SIZE}")
    print("="*80)
    
    if not CONTEXT_FILE.exists():
        raise FileNotFoundError(f"Context file not found: {CONTEXT_FILE}")
    
    content = CONTEXT_FILE.read_text(encoding="utf-8")
    print(f"ðŸ“„ Loaded context: {len(content):,} characters\n")
    
    # Initialize agent pool
    pool = EnhancedAgentPool()
    
    # Get agent IDs
    agent_id_kaoz = os.environ.get("AGENT_ID_KAOZ", "asst_72uzK1Yt2hsu2qVyt22NkMiO")
    agent_id_order = os.environ.get("AGENT_ID_ORDER", "asst_Pgb3ctXzbsv21gX2auBeEFZx")
    
    # Calculate agent distribution (250 of each type for 500x)
    agents_per_type = MAX_PARALLEL_AGENTS // 2
    
    start_time = time.time()
    
    # Launch both agent swarms in parallel
    kaoz_task = run_agent_swarm(pool, "KAOZ", agent_id_kaoz, content, agents_per_type)
    order_task = run_agent_swarm(pool, "ORDER", agent_id_order, content, agents_per_type)
    
    kaoz_metrics, order_metrics = await asyncio.gather(kaoz_task, order_task)
    
    all_metrics = kaoz_metrics + order_metrics
    total_time = time.time() - start_time
    
    # Save metrics
    save_metrics(all_metrics)
    
    # Print summary
    print("\n" + "="*80)
    print("ðŸŽ‰ EXECUTION COMPLETE")
    print("="*80)
    print(f"Total Agents: {len(all_metrics)}")
    print(f"Successful: {sum(1 for m in all_metrics if m.success)}")
    print(f"Failed: {sum(1 for m in all_metrics if not m.success)}")
    print(f"Total Messages Sent: {sum(m.messages_sent for m in all_metrics):,}")
    print(f"Total Messages Received: {sum(m.messages_received for m in all_metrics):,}")
    print(f"Total Duration: {total_time:.2f}s")
    print(f"Throughput: {len(all_metrics) / total_time:.2f} agents/second")
    print(f"Check {OUT_DIR} for results!")
    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
